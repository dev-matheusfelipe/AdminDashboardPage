{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtcUMz7AvpfzUTG6x7M4gP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dev-matheusfelipe/AdminDashboardPage/blob/main/kpe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extração de frases-chave a partir de textos.\n",
        "\n",
        "Passo 1: Instalação das bibliotecas.."
      ],
      "metadata": {
        "id": "1JUHgMPF42Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import pkg_resources,imp\n",
        "\n",
        "imp.reload(pkg_resources)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2plILCeM42qG",
        "outputId": "76c1781e-b8e5-43d7-9a68-075447288571"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-mtl4ht3x\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-mtl4ht3x\n",
            "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (3.9.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.6.1)\n",
            "Collecting unidecode (from pke==2.0.0)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (1.4.2)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.11/dist-packages (from pke==2.0.0) (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->pke==2.0.0) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->pke==2.0.0) (2024.11.6)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pke==2.0.0) (3.6.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.2.3->pke==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.2.3->pke==2.0.0) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (0.1.2)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pke\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160628 sha256=2f6b45cd1d489ecfd625ed4bdb089ae6b384144e4a1550f3d87e25a4bc20fcf5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6z92frth/wheels/b2/b9/39/32b734e95546f83e8d2584c49db5bf486b1ac093a94aef6f33\n",
            "Successfully built pke\n",
            "Installing collected packages: unidecode, pke\n",
            "Successfully installed pke-2.0.0 unidecode-1.3.8\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-8b4c472bdc67>:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources,imp\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'pkg_resources' from '/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 2: Importando a lib pke e inicializando\n",
        "o modelo de extração"
      ],
      "metadata": {
        "id": "e1GeB6oB48kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pke\n",
        "\n",
        "# initialize a TopicRank keyphrase extraction model\n",
        "\n",
        "extractor = pke.unsupervised.TopicRank()"
      ],
      "metadata": {
        "id": "xBscNXdy49JO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 3: Definindo e carregando o texto\n",
        "a ser analisado."
      ],
      "metadata": {
        "id": "vAjfsHKO5G0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"\"\"Tesla has been ordered to recall nearly 4,000 of its Cybertrucks due to\n",
        "an accelerator pedal that\n",
        "\n",
        "can stick in place when pressed down.\n",
        "\n",
        "The cause, according to the regulator: soap.\n",
        "\n",
        "“An unapproved change introduced lubricant (soap) to aid in the component\n",
        "assembly of the pad onto the accelerator pedal.\n",
        "\n",
        "Residual lubricant reduced the retention of the pad to the pedal,” the NHTSA\n",
        "wrote in the recall document.\n",
        "\n",
        "Tesla has yet to detail how many of the futuristic looking Cybertrucks it has\n",
        "produced. But it has said that it would\n",
        "\n",
        "be slow ramping up production of the vehicle, which had its first deliveries in late\n",
        "November.\n",
        "\n",
        "The NHTSA said the recall affects “all Model Year (‘MY’) 2024 Cybertruck vehicles\n",
        "manufactured from November 13, 2023, to April 4, 2024.”\n",
        "\n",
        "\"\"\".replace(\"\\n\", \" \")\n",
        "\n",
        "extractor.load_document(input=sample, language='en')"
      ],
      "metadata": {
        "id": "CrYb4_sb5Iqm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 4: Imprimindo as informações das\n",
        "sentenças"
      ],
      "metadata": {
        "id": "0KWxfj8t5K82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(extractor.sentences):\n",
        "    # print out the sentence id, its tokens, its stems and the corresponding Part-of-Speech tags\n",
        "    print(\"sentence {}:\".format(i))\n",
        "    print(\" - words: {} ...\".format(' '.join(sentence.words[:5])))\n",
        "    print(\" - stems: {} ...\".format(' '.join(sentence.stems[:5])))\n",
        "    print(\" - PoS: {} ...\".format(' '.join(sentence.pos[:5])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD_802Uv5Lb2",
        "outputId": "71ab2fba-1948-42b5-83a1-ed676a1d9685"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence 0:\n",
            " - words: Tesla has been ordered to ...\n",
            " - stems: tesla ha been order to ...\n",
            " - PoS: PROPN AUX AUX VERB PART ...\n",
            "sentence 1:\n",
            " - words:   The cause , according ...\n",
            " - stems:   the caus , accord ...\n",
            " - PoS: SPACE DET NOUN PUNCT VERB ...\n",
            "sentence 2:\n",
            " - words:   “ An unapproved change ...\n",
            " - stems:   “ an unapprov chang ...\n",
            " - PoS: SPACE PUNCT DET ADJ NOUN ...\n",
            "sentence 3:\n",
            " - words:   Residual lubricant reduced the ...\n",
            " - stems:   residu lubric reduc the ...\n",
            " - PoS: SPACE ADJ NOUN VERB DET ...\n",
            "sentence 4:\n",
            " - words:   Tesla has yet to ...\n",
            " - stems:   tesla ha yet to ...\n",
            " - PoS: SPACE PROPN AUX ADV PART ...\n",
            "sentence 5:\n",
            " - words: But it has said that ...\n",
            " - stems: but it ha said that ...\n",
            " - PoS: CCONJ PRON AUX VERB SCONJ ...\n",
            "sentence 6:\n",
            " - words:   The NHTSA said the ...\n",
            " - stems:   the nhtsa said the ...\n",
            " - PoS: SPACE DET PROPN VERB DET ...\n",
            "sentence 7:\n",
            " - words:   ...\n",
            " - stems:   ...\n",
            " - PoS: SPACE ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 5: Identificando as frases-\n",
        "chave candidatas"
      ],
      "metadata": {
        "id": "5buyqyFA5dHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extractor.candidate_selection()\n",
        "\n",
        "for i, candidate in enumerate(extractor.candidates):\n",
        "\n",
        "\n",
        "\n",
        "    # print out the candidate id, its stemmed form\n",
        "\n",
        "    print(\"candidate {}: {} (stemmed form)\".format(i, candidate))\n",
        "\n",
        "\n",
        "\n",
        "    # print out the surface forms of the candidate\n",
        "\n",
        "    print(\" - surface forms:\", [ \" \".join(u) for u in\n",
        "extractor.candidates[candidate].surface_forms])\n",
        "\n",
        "\n",
        "\n",
        "    # print out the corresponding offsets\n",
        "\n",
        "    print(\" - offsets:\", extractor.candidates[candidate].offsets)\n",
        "\n",
        "\n",
        "\n",
        "    # print out the corresponding sentence ids\n",
        "\n",
        "    print(\" - sentence_ids:\", extractor.candidates[candidate].sentence_ids)\n",
        "\n",
        "\n",
        "\n",
        "    # print out the corresponding PoS patterns\n",
        "\n",
        "    print(\" - pos_patterns:\", extractor.candidates[candidate].pos_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgDmhYfK5dlG",
        "outputId": "146c2d33-8902-4d10-bdcd-90d1c3b7747e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "candidate 0: tesla (stemmed form)\n",
            " - surface forms: ['Tesla', 'Tesla']\n",
            " - offsets: [0, 84]\n",
            " - sentence_ids: [0, 4]\n",
            " - pos_patterns: [['PROPN'], ['PROPN']]\n",
            "candidate 1: cybertruck (stemmed form)\n",
            " - surface forms: ['Cybertrucks', 'Cybertrucks']\n",
            " - offsets: [10, 95]\n",
            " - sentence_ids: [0, 4]\n",
            " - pos_patterns: [['NOUN'], ['NOUN']]\n",
            "candidate 2: acceler pedal (stemmed form)\n",
            " - surface forms: ['accelerator pedal', 'accelerator pedal']\n",
            " - offsets: [14, 58]\n",
            " - sentence_ids: [0, 2]\n",
            " - pos_patterns: [['ADJ', 'NOUN'], ['ADJ', 'NOUN']]\n",
            "candidate 3: place (stemmed form)\n",
            " - surface forms: ['place']\n",
            " - offsets: [21]\n",
            " - sentence_ids: [0]\n",
            " - pos_patterns: [['NOUN']]\n",
            "candidate 4: caus (stemmed form)\n",
            " - surface forms: ['cause']\n",
            " - offsets: [28]\n",
            " - sentence_ids: [1]\n",
            " - pos_patterns: [['NOUN']]\n",
            "candidate 5: regul (stemmed form)\n",
            " - surface forms: ['regulator']\n",
            " - offsets: [33]\n",
            " - sentence_ids: [1]\n",
            " - pos_patterns: [['NOUN']]\n",
            "candidate 6: soap (stemmed form)\n",
            " - surface forms: ['soap', 'soap']\n",
            " - offsets: [35, 45]\n",
            " - sentence_ids: [1, 2]\n",
            " - pos_patterns: [['NOUN'], ['NOUN']]\n",
            "candidate 7: unapprov chang (stemmed form)\n",
            " - surface forms: ['unapproved change']\n",
            " - offsets: [40]\n",
            " - sentence_ids: [2]\n",
            " - pos_patterns: [['ADJ', 'NOUN']]\n",
            "candidate 8: lubric (stemmed form)\n",
            " - surface forms: ['lubricant']\n",
            " - offsets: [43]\n",
            " - sentence_ids: [2]\n",
            " - pos_patterns: [['NOUN']]\n",
            "candidate 9: compon assembl (stemmed form)\n",
            " - surface forms: ['component assembly']\n",
            " - offsets: [51]\n",
            " - sentence_ids: [2]\n",
            " - pos_patterns: [['NOUN', 'NOUN']]\n",
            "candidate 10: pad (stemmed form)\n",
            " - surface forms: ['pad', 'pad']\n",
            " - offsets: [55, 69]\n",
            " - sentence_ids: [2, 3]\n",
            " - pos_patterns: [['NOUN'], ['NOUN']]\n",
            "candidate 11: residu lubric (stemmed form)\n",
            " - surface forms: ['Residual lubricant']\n",
            " - offsets: [62]\n",
            " - sentence_ids: [3]\n",
            " - pos_patterns: [['ADJ', 'NOUN']]\n",
            "candidate 12: retent (stemmed form)\n",
            " - surface forms: ['retention']\n",
            " - offsets: [66]\n",
            " - sentence_ids: [3]\n",
            " - pos_patterns: [['NOUN']]\n",
            "candidate 13: pedal (stemmed form)\n",
            " - surface forms: ['pedal']\n",
            " - offsets: [72]\n",
            " - sentence_ids: [3]\n",
            " - pos_patterns: [['NOUN']]\n",
            "candidate 14: nhtsa (stemmed form)\n",
            " - surface forms: ['NHTSA', 'NHTSA']\n",
            " - offsets: [76, 128]\n",
            " - sentence_ids: [3, 6]\n",
            " - pos_patterns: [['PROPN'], ['PROPN']]\n",
            "candidate 15: recal document (stemmed form)\n",
            " - surface forms: ['recall document']\n",
            " - offsets: [80]\n",
            " - sentence_ids: [3]\n",
            " - pos_patterns: [['NOUN', 'NOUN']]\n",
            "candidate 16: futurist (stemmed form)\n",
            " - surface forms: ['futuristic']\n",
            " - offsets: [93]\n",
            " - sentence_ids: [4]\n",
            " - pos_patterns: [['ADJ']]\n",
            "candidate 17: slow (stemmed form)\n",
            " - surface forms: ['slow']\n",
            " - offsets: [109]\n",
            " - sentence_ids: [5]\n",
            " - pos_patterns: [['ADJ']]\n",
            "candidate 18: product (stemmed form)\n",
            " - surface forms: ['production']\n",
            " - offsets: [112]\n",
            " - sentence_ids: [5]\n",
            " - pos_patterns: [['NOUN']]\n",
            "candidate 19: vehicl (stemmed form)\n",
            " - surface forms: ['vehicle']\n",
            " - offsets: [115]\n",
            " - sentence_ids: [5]\n",
            " - pos_patterns: [['NOUN']]\n",
            "candidate 20: late novemb (stemmed form)\n",
            " - surface forms: ['late November']\n",
            " - offsets: [123]\n",
            " - sentence_ids: [5]\n",
            " - pos_patterns: [['ADJ', 'PROPN']]\n",
            "candidate 21: recal (stemmed form)\n",
            " - surface forms: ['recall']\n",
            " - offsets: [131]\n",
            " - sentence_ids: [6]\n",
            " - pos_patterns: [['NOUN']]\n",
            "candidate 22: model year (stemmed form)\n",
            " - surface forms: ['Model Year']\n",
            " - offsets: [135]\n",
            " - sentence_ids: [6]\n",
            " - pos_patterns: [['PROPN', 'PROPN']]\n",
            "candidate 23: cybertruck vehicl (stemmed form)\n",
            " - surface forms: ['Cybertruck vehicles']\n",
            " - offsets: [143]\n",
            " - sentence_ids: [6]\n",
            " - pos_patterns: [['PROPN', 'NOUN']]\n",
            "candidate 24: novemb (stemmed form)\n",
            " - surface forms: ['November']\n",
            " - offsets: [147]\n",
            " - sentence_ids: [6]\n",
            " - pos_patterns: [['PROPN']]\n",
            "candidate 25: april (stemmed form)\n",
            " - surface forms: ['April']\n",
            " - offsets: [153]\n",
            " - sentence_ids: [6]\n",
            " - pos_patterns: [['PROPN']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 6: Ranqueando as palavras-\n",
        "chave candidatas"
      ],
      "metadata": {
        "id": "HNpZeiQb5gvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extractor.candidate_weighting()\n",
        "\n",
        "for i, topic in enumerate(extractor.topics):\n",
        "\n",
        "\n",
        "\n",
        "    # print out the topic id and the candidates it groups together\n",
        "\n",
        "    print(\"topic {}: {} \".format(i, ';'.join(topic)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWQmiocm5iaG",
        "outputId": "ffc8480f-8dac-40e2-90ae-d586606fa553"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic 0: cybertruck;cybertruck vehicl \n",
            "topic 1: vehicl \n",
            "topic 2: lubric;residu lubric \n",
            "topic 3: recal;recal document \n",
            "topic 4: acceler pedal;pedal \n",
            "topic 5: late novemb;novemb \n",
            "topic 6: april \n",
            "topic 7: caus \n",
            "topic 8: compon assembl \n",
            "topic 9: futurist \n",
            "topic 10: model year \n",
            "topic 11: nhtsa \n",
            "topic 12: pad \n",
            "topic 13: place \n",
            "topic 14: product \n",
            "topic 15: regul \n",
            "topic 16: retent \n",
            "topic 17: slow \n",
            "topic 18: soap \n",
            "topic 19: tesla \n",
            "topic 20: unapprov chang \n"
          ]
        }
      ]
    }
  ]
}